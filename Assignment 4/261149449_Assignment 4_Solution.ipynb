{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\Lakshya\n",
      "[nltk_data]     Agarwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from rich import print\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"twitter_samples\")\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^d\\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^d \\beta_j^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical estimation of $\\beta$\n",
    "\n",
    "Expressed in matrix format, the loss function can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\beta) = (y - X\\beta)^T(y - X\\beta) + \\lambda (\\beta^T\\beta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $X$ is a $n \\times d+1$ matrix ($d$ dimensions and the intercept term)\n",
    "\n",
    "Taking the derivative with respect to $\\beta$ and setting it to zero,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\beta)}{\\partial \\beta} &= -2X^T(y-X\\beta) + 2\\lambda\\beta \\\\\n",
    "0 &= -2X^T(y-X\\beta) + 2\\lambda\\beta \\\\\n",
    "2\\lambda\\beta &= 2X^T(y-X\\beta) \\\\\n",
    "\\lambda \\beta &= X^Ty - X^TX\\beta \\\\\n",
    "(\\lambda I + X^TX)\\beta  &= X^Ty \\\\\n",
    "\\beta &= (\\lambda I + X^TX)^{-1} X^Ty\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, the analytical solution for $\\beta$ here, after a bit of matrix algebra, comes out to be similar to the OLS solution but with the addition of the $\\lambda$ term as a regularization parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the loss function $\\nabla L(\\beta)$\n",
    "\n",
    "The gradient $L(\\beta)$ is the vector of partial derivatives for each $B_j$, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\nabla L(\\beta) &= \\begin{bmatrix}\n",
    "\\frac{\\partial L(\\beta)}{\\partial \\beta_0}\n",
    "        \\\\ \\frac{\\partial L(\\beta)}{\\partial \\beta_1}\n",
    "        \\\\ \\vdots\n",
    "        \\\\ \\frac{\\partial L(\\beta)}{\\partial \\beta_d}\n",
    "        \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Solving each expression and simplifying to matrix notation, we get the same result as above:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla L(\\beta) &= -2X^T(y-X\\beta) + 2\\lambda\\beta \\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update step\n",
    "\n",
    "The update step for the gradient descent, assuming a constant learning rate of $\\gamma$, becomes:\n",
    "\n",
    "$$\n",
    "\\beta_{t+1} = \\beta_{t} - \\gamma \\nabla L(\\beta_{t})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedo-code for stochastic gradient descent\n",
    "\n",
    "Input: Initial guess $\\beta_I$, learning rate of $\\gamma_t$\n",
    "\n",
    "For: $t = 0, 1, \\dots, T-1$, do\n",
    "\n",
    "> Choose $i \\in {1, 2, \\dots, n}$ unifoirmly at random\n",
    "\n",
    "> $\\beta_{t+1} = \\beta_{t} - \\gamma_t \\nabla L_i(\\beta_t)$\n",
    "\n",
    "end For loop\n",
    "\n",
    "return $\\beta_T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7000</span>,<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m7000\u001b[0m,\u001b[1m)\u001b[0m\n",
       "\u001b[1m(\u001b[0m\u001b[1;36m7000\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "all_negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "labels = np.append(\n",
    "    np.ones((len(all_positive_tweets), 1)),\n",
    "    np.zeros((len(all_negative_tweets), 1)),\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "X = np.array(tweets)\n",
    "y = np.array(labels)\n",
    "\n",
    "indices = np.random.choice(range(len(X)), size=7000, replace=False)\n",
    "\n",
    "X_sampled = X[indices]\n",
    "y_sampled = y[indices]\n",
    "\n",
    "print(X_sampled.shape, y_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Positive tweet examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Positive tweet examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Stats for the day have arrived. 2 new followers and NO unfollowers :) via http://t.co/IGc54dj3fJ.'</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'@PippaLov @twinesocial Thank you very much Pippa :-)'</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'Be Online mamaya LoiYals para sa Good News.. :-)\\n\\nTruthfulWordsOf BeaNatividad\\n#NKNKKPagpapakumbaba'</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'@uptownctw P£T£, Heiya, visit my site if u wanna grab a FREE 30.000 8 Ball Pool Coins. Thank you :)'</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'@itsbecca65 thanks becca :)'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'Stats for the day have arrived. 2 new followers and NO unfollowers :\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via http://t.co/IGc54dj3fJ.'\u001b[0m\n",
       " \u001b[32m'@PippaLov @twinesocial Thank you very much Pippa :-\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       " \u001b[32m'Be Online mamaya LoiYals para sa Good News.. :-\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nTruthfulWordsOf BeaNatividad\\n#NKNKKPagpapakumbaba'\u001b[0m\n",
       " \u001b[32m'@uptownctw P£T£, Heiya, visit my site if u wanna grab a FREE 30.000 8 Ball Pool Coins. Thank you :\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       " \u001b[32m'@itsbecca65 thanks becca :\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Negative tweet examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Negative tweet examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'i miss watching anna akana videos :('</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">\"@st3cav @MCFC @sterling31 @YayaToure True, deosn't help that Liverpool were here last week, most have never been </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Pool though :(\"</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">\"Its 6:15 and I'm wide awake :(\"</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'when ed sheeran is preforming in your country tonight and ur not going :('</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'Im hungry now :('</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'i miss watching anna akana videos :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\n",
       " \u001b[32m\"@st3cav @MCFC @sterling31 @YayaToure True, deosn't help that Liverpool were here last week, most have never been \u001b[0m\n",
       "\u001b[32mto Pool though :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"\u001b[0m\n",
       " \u001b[32m\"Its 6:15 and I'm wide awake :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"\u001b[0m\n",
       " \u001b[32m'when ed sheeran is preforming in your country tonight and ur not going :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\n",
       " \u001b[32m'Im hungry now :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looking at sample tweets\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(\"Positive tweet examples\")\n",
    "print(\"-\" * 100)\n",
    "print(X_sampled[y_sampled.flatten() == 1][:5])\n",
    "print(\"-\" * 100)\n",
    "print(\"Negative tweet examples\")\n",
    "print(\"-\" * 100)\n",
    "print(X_sampled[y_sampled.flatten() == 0][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data\n",
    "\n",
    "- Tokenizing the string\n",
    "- Convert the tweet into lowercase and split the tweets into tokens(words)\n",
    "- Removing stop words and punctuation\n",
    "- Removing commonly used words on the twitter platform like the hashtag, retweet marks, hyperlinks, numbers, and email address\n",
    "- Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twitter_Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stopwords_english = stopwords.words(\"english\")\n",
    "        self.tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True\n",
    "        )\n",
    "        self.punctuation = string.punctuation\n",
    "\n",
    "    def remove_chars(self, tweet):\n",
    "        # remove old style retweet text \"RT\"\n",
    "        tweet = re.sub(r\"^RT[\\s]+\", \"\", tweet)\n",
    "\n",
    "        # remove hyperlinks\n",
    "        tweet = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\", \"\", tweet)\n",
    "\n",
    "        # remove hashtags\n",
    "        tweet = re.sub(r\"#\", \"\", tweet)\n",
    "\n",
    "        # remove email addresses\n",
    "        tweet = re.sub(r\"\\S*@\\S*\\s?\", \"\", tweet)\n",
    "\n",
    "        # remove numbers\n",
    "        tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
    "\n",
    "        return tweet\n",
    "\n",
    "    def tokenize(self, tweet):\n",
    "        # tokenize tweets\n",
    "        return self.tokenizer.tokenize(tweet)\n",
    "\n",
    "    def remove_stopwords(self, tweet_tokens):\n",
    "        tweets_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if word in self.stopwords_english or word in self.punctuation:\n",
    "                continue\n",
    "\n",
    "            tweets_clean.append(word)\n",
    "\n",
    "        return tweets_clean\n",
    "\n",
    "    def stem(self, tweet_tokens):\n",
    "        tweets_stem = []\n",
    "        for word in tweet_tokens:\n",
    "            stem_word = self.stemmer.stem(word)\n",
    "            tweets_stem.append(stem_word)\n",
    "\n",
    "        return tweets_stem\n",
    "\n",
    "    def process_tweet(self, tweet):\n",
    "        tweet = self.remove_chars(tweet)\n",
    "        tweet_tokens = self.tokenize(tweet)\n",
    "        tweet_tokens = self.remove_stopwords(tweet_tokens)\n",
    "        tweet_tokens = self.stem(tweet_tokens)\n",
    "\n",
    "        return tweet_tokens\n",
    "\n",
    "    def process_tweets(self, tweets):\n",
    "        processed_tweets = []\n",
    "\n",
    "        for _, tweet in tqdm(enumerate(tweets)):\n",
    "            processed_tweets.append(self.process_tweet(tweet))\n",
    "\n",
    "        return processed_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7000it [00:01, 4033.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Samples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Samples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'i miss watching anna akana videos :('</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">\"@st3cav @MCFC @sterling31 @YayaToure True, deosn't help that Liverpool were here last week, most have never been </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Pool though :(\"</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">\"Its 6:15 and I'm wide awake :(\"</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'when ed sheeran is preforming in your country tonight and ur not going :('</span>\n",
       " <span style=\"color: #008000; text-decoration-color: #008000\">'Im hungry now :('</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'i miss watching anna akana videos :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\n",
       " \u001b[32m\"@st3cav @MCFC @sterling31 @YayaToure True, deosn't help that Liverpool were here last week, most have never been \u001b[0m\n",
       "\u001b[32mto Pool though :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"\u001b[0m\n",
       " \u001b[32m\"Its 6:15 and I'm wide awake :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"\u001b[0m\n",
       " \u001b[32m'when ed sheeran is preforming in your country tonight and ur not going :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\n",
       " \u001b[32m'Im hungry now :\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Processed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Processed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'miss'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'watch'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'anna'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'akana'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'video'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">':('</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'true'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"deosn't\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'help'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'liverpool'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'last'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'week'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'never'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pool'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'though'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">':('</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"i'm\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'wide'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'awak'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">':('</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'ed'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sheeran'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'preform'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'countri'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tonight'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ur'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'go'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">':('</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'im'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hungri'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">':('</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m[\u001b[0m\u001b[32m'miss'\u001b[0m, \u001b[32m'watch'\u001b[0m, \u001b[32m'anna'\u001b[0m, \u001b[32m'akana'\u001b[0m, \u001b[32m'video'\u001b[0m, \u001b[32m':\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[32m'true'\u001b[0m, \u001b[32m\"deosn't\"\u001b[0m, \u001b[32m'help'\u001b[0m, \u001b[32m'liverpool'\u001b[0m, \u001b[32m'last'\u001b[0m, \u001b[32m'week'\u001b[0m, \u001b[32m'never'\u001b[0m, \u001b[32m'pool'\u001b[0m, \u001b[32m'though'\u001b[0m, \u001b[32m':\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[32m\"i'm\"\u001b[0m, \u001b[32m'wide'\u001b[0m, \u001b[32m'awak'\u001b[0m, \u001b[32m':\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[32m'ed'\u001b[0m, \u001b[32m'sheeran'\u001b[0m, \u001b[32m'preform'\u001b[0m, \u001b[32m'countri'\u001b[0m, \u001b[32m'tonight'\u001b[0m, \u001b[32m'ur'\u001b[0m, \u001b[32m'go'\u001b[0m, \u001b[32m':\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[32m'im'\u001b[0m, \u001b[32m'hungri'\u001b[0m, \u001b[32m':\u001b[0m\u001b[32m(\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = Twitter_Preprocessor()\n",
    "\n",
    "X_sampled_processed = preprocessor.process_tweets(X_sampled)\n",
    "print(\"Samples\")\n",
    "print(X_sampled[:5])\n",
    "print(\"Processed\")\n",
    "print(X_sampled_processed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction\n",
    "\n",
    "To predict the sentiment of a tweet, we will build a logistic regression model. The features that will be used as inputs to the model are:\n",
    "\n",
    "- **Bias term:** This is a constant term that is added to the feature vector, represented as $x_0$\n",
    "- **Number of positive words in the tweet:** This is a count of the number of positive words (defined through a bag-of-words model) in the tweet, represented as $x_1$\n",
    "- **Number of negative words in the tweet:** This is a count of the number of negative words (defined through a bag-of-words model) in the tweet, represented as $x_2$\n",
    "- **Number of words in the tweet**: This is a  count of the number of words in the tweet, represented as $x_3$\n",
    "\n",
    "To build these features, we use a Bag of Words (BoW) model to count the number of times each word appears for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag_of_words(tweets, labels):\n",
    "    word_freq = {}\n",
    "\n",
    "    for tweet, label in list(zip(tweets, labels)):\n",
    "        for word in tweet:\n",
    "            label = int(label)\n",
    "            word_freq[(word, label)] = word_freq.get((word, label), 0) + 1\n",
    "\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "bow_dict = build_bag_of_words(X_sampled_processed, y_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, bow_dict):\n",
    "    features = np.zeros((1, 4))\n",
    "\n",
    "    # bias term is set to 1\n",
    "    features[0, 0] = 1\n",
    "\n",
    "    for word in tweet:\n",
    "        # Positive word count\n",
    "        features[0, 1] += bow_dict.get((word, 1), 0)\n",
    "\n",
    "        # Negative word count\n",
    "        features[0, 2] += bow_dict.get((word, 0), 0)\n",
    "\n",
    "    # Log of total word count\n",
    "    features[0, 3] = len(tweet)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Train set: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4200</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Train set: \u001b[1;36m4200\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Test set: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2800</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Test set: \u001b[1;36m2800\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_X_tweet, test_X_tweet, train_y_tweet, test_y_tweet = train_test_split(\n",
    "    X_sampled_processed, y_sampled, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(train_X_tweet)}\")\n",
    "print(f\"Test set: {len(test_X_tweet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Train X: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4200</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Train X: \u001b[1m(\u001b[0m\u001b[1;36m4200\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Test X: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2800</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Test X: \u001b[1m(\u001b[0m\u001b[1;36m2800\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the feature matrix for the training set\n",
    "train_X = np.zeros((len(train_X_tweet), 4))\n",
    "for i, tweet in enumerate(train_X_tweet):\n",
    "    train_X[i, :] = extract_features(tweet, bow_dict)\n",
    "\n",
    "# Build the feature matrix for the test set\n",
    "test_X = np.zeros((len(test_X_tweet), 4))\n",
    "for i, tweet in enumerate(test_X_tweet):\n",
    "    test_X[i, :] = extract_features(tweet, bow_dict)\n",
    "\n",
    "print(f\"Train X: {train_X.shape}\")\n",
    "print(f\"Test X: {test_X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black-box classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training blackbox model on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4200</span> samples<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Training blackbox model on \u001b[1;36m4200\u001b[0m samples\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Coefficents: <span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.15564283</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01335474</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.01173232</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.09416597</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Coefficents: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.15564283\u001b[0m  \u001b[1;36m0.01335474\u001b[0m \u001b[1;36m-0.01173232\u001b[0m \u001b[1;36m-0.09416597\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating blackbox model on <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2800</span> samples<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating blackbox model on \u001b[1;36m2800\u001b[0m samples\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9892857142857143</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accuracy: \u001b[1;36m0.9892857142857143\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Precision: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.980528511821975</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Precision: \u001b[1;36m0.980528511821975\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9985835694050992</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall: \u001b[1;36m0.9985835694050992\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">F1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9894736842105264</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "F1: \u001b[1;36m0.9894736842105264\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "blackbox_model = LogisticRegression(random_state=42, max_iter=1000, fit_intercept=False)\n",
    "\n",
    "\n",
    "print(f\"Training blackbox model on {train_X.shape[0]} samples...\")\n",
    "\n",
    "blackbox_model.fit(train_X, train_y_tweet.ravel())\n",
    "\n",
    "print(f\"Coefficents: {blackbox_model.coef_}\")\n",
    "\n",
    "print(f\"Evaluating blackbox model on {test_X.shape[0]} samples...\")\n",
    "\n",
    "test_y_pred = blackbox_model.predict(test_X)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(test_y_tweet, test_y_pred)}\")\n",
    "\n",
    "print(f\"Precision: {precision_score(test_y_tweet, test_y_pred)}\")\n",
    "\n",
    "print(f\"Recall: {recall_score(test_y_tweet, test_y_pred)}\")\n",
    "\n",
    "print(f\"F1: {f1_score(test_y_tweet, test_y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical formulation\n",
    "\n",
    "Given a tweet $x$ (I just know Elon Musk is smiling somewhere!), the probability of the tweet being positive ($y=1$) is given by:\n",
    "\n",
    "$$\n",
    "P(y=1|x, \\beta) = \\sigma(\\beta \\cdot x)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\sigma$ is the sigmoid function, defined as:\n",
    "\n",
    "  $$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "\n",
    "- $\\beta$ is the parameter vector $[\\beta_0, \\beta_1, \\beta_2]$\n",
    "\n",
    "Therefore, the probability function can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(y=1|x, \\beta) & &= \\sigma(\\beta \\cdot x) &= \\frac{1}{1 + e^{-\\beta \\cdot x}} \\\\\n",
    "P(y=0|x, \\beta) &= 1 - P(y=1|x, \\beta) &= \\sigma(-\\beta \\cdot x) &= \\frac{1}{1 + e^{\\beta \\cdot x}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The vector $\\beta$ is learned from the training data by minimizing the negative log-likelihood loss function, and then used to predict the sentiment of the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood function\n",
    "\n",
    "The likelihood function for the logistic regression model is the likelihood of observing labels $y_i$ given the features $x_i$ and the parameters $\\beta$. It is defined as the product of individual probabilities for each observation:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^n P(y_i | x_i, \\beta) = \\prod_{i=1}^n (\\sigma(\\beta \\cdot x_i))^{y_i} (1 - \\sigma(\\beta \\cdot x_i))^{1 - y_i}\n",
    "$$\n",
    "\n",
    "The estimated probability can be written as:\n",
    "\n",
    "$$\n",
    "p_i = \\sigma(\\beta \\cdot x_i)\n",
    "$$\n",
    "\n",
    "In practice, it is easier to work with the log-likelihood function, which is the logarithm of the likelihood function:\n",
    "\n",
    "$$\n",
    "\\log L(\\beta) = \\sum_{i=1}^n y_i \\log p_i + (1 - y_i) \\log (1 - p_i)\n",
    "$$\n",
    "\n",
    "To optimize, we consider the negative log-likelihood function, and minimize it:\n",
    "\n",
    "$$\n",
    "\\text{NLL} = -\\sum_{i=1}^n \\left[ y_i \\log (\\sigma(\\beta \\cdot x_i)) + (1 - y_i) \\log (1 - \\sigma(\\beta \\cdot x_i)) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimzing the negative log-likelihood function using stochastic gradient descent\n",
    "\n",
    "#### Gradient\n",
    "\n",
    "The gradient of the negative log-likelihood function is given by:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} \\text{NLL} = -(y - \\sigma(\\beta \\cdot x))x\n",
    "$$\n",
    "\n",
    "#### Objective function\n",
    "\n",
    "Given samples $S = {(x^{(i)}, y^{(i)})}_{i=1}^n$, the objective is to find $\\beta$ that minimizes the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "J_S(\\beta) = -\\frac{1}{n} \\sum_{i=1}^n \\log P(y^{(i)} | x^{(i)}, \\beta)\n",
    "$$\n",
    "\n",
    "The gradient becomes:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} J_S(\\beta) = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - \\sigma(\\beta \\cdot x^{(i)}))x^{(i)}\n",
    "$$\n",
    "\n",
    "#### Stochastic gradient descent\n",
    "\n",
    "The steps for the stochastic gradient descent algorithm are as follows:\n",
    "\n",
    "- Initialize $\\beta$ to some value $\\beta^0$\n",
    "- For $t = 0, 1, \\dots, T-1$, do\n",
    "  - Choose a sample $i \\in {1, 2, \\dots, n}$ uniformly at random\n",
    "  - $\\beta^{t+1} = \\beta^t + \\eta \\nabla_{\\beta} J_S^{i}(\\beta^t)$\n",
    "- Return $\\beta^T$\n",
    "\n",
    "where $\\eta$ is the learning rate, and $T$ is the number of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def prediction_function(X, beta):\n",
    "    return sigmoid(np.dot(X, beta))\n",
    "\n",
    "\n",
    "def loss_function(y, x, beta):\n",
    "    predictions = prediction_function(x, beta)\n",
    "    predictions = np.clip(predictions, 1e-9, 1 - 1e-9)\n",
    "\n",
    "    return -y * np.log(predictions) - (1 - y) * np.log(1 - predictions)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "    X, y, beta_init, learning_rate=0.0001, epochs=1000, tolerance=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent to optimize the beta coefficients for a logistic regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): The input features matrix, where each row represents a sample and each column represents a feature.\n",
    "    - y (numpy.ndarray): The target vector, where each element is the target for a sample.\n",
    "    - beta_init (numpy.ndarray): The initial values for the beta coefficients.\n",
    "    - learning_rate (float, optional): The learning rate for the optimization. Defaults to 0.0001.\n",
    "    - epochs (int, optional): The number of iterations to run the gradient descent. Defaults to 1000.\n",
    "    - tolerance (float, optional): The tolerance for the loss to declare convergence. Defaults to 1e-6.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The optimized beta coefficients after running stochastic gradient descent.\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "\n",
    "    beta = beta_init\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, n_samples)\n",
    "        X_i = X[idx, :].reshape(1, -1)\n",
    "        y_i = y[idx]\n",
    "\n",
    "        predictions = prediction_function(X_i, beta)\n",
    "        errors = y_i - predictions\n",
    "        gradient = np.dot(X_i.T, errors)\n",
    "\n",
    "        beta += learning_rate * gradient\n",
    "\n",
    "        loss = loss_function(y, X, beta).mean()\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Loss at epoch {epoch}: {loss}\")\n",
    "\n",
    "        if loss < tolerance:\n",
    "            break\n",
    "\n",
    "    print(f\"Final loss: {loss_history[-1]}\")\n",
    "\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.467647052486779</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m0\u001b[0m: \u001b[1;36m7.467647052486779\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-bbcb9c1d1090>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13296266687064653</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m1000\u001b[0m: \u001b[1;36m0.13296266687064653\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2000</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.13305149926469398</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m2000\u001b[0m: \u001b[1;36m0.13305149926469398\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3000</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12646889123556485</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m3000\u001b[0m: \u001b[1;36m0.12646889123556485\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4000</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4105158054144747</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m4000\u001b[0m: \u001b[1;36m0.4105158054144747\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5000</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12516747116714597</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m5000\u001b[0m: \u001b[1;36m0.12516747116714597\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6000</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1626469606683027</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m6000\u001b[0m: \u001b[1;36m0.1626469606683027\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7000</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14329670550204843</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m7000\u001b[0m: \u001b[1;36m0.14329670550204843\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8000</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1574067885720314</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m8000\u001b[0m: \u001b[1;36m0.1574067885720314\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loss at epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9000</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1549820472821086</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loss at epoch \u001b[1;36m9000\u001b[0m: \u001b[1;36m0.1549820472821086\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14079260090181248</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Final loss: \u001b[1;36m0.14079260090181248\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.02739916\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[1m[\u001b[0m \u001b[1;36m2.92325763\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[1m[\u001b[0m\u001b[1;36m-2.10764575\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[1m[\u001b[0m \u001b[1;36m0.12694995\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_T = stochastic_gradient_descent(\n",
    "    train_X,\n",
    "    train_y_tweet,\n",
    "    np.zeros((train_X.shape[1], 1)),\n",
    "    learning_rate=0.001,\n",
    "    epochs=10000,\n",
    "    tolerance=1e-4,\n",
    ")\n",
    "\n",
    "beta_T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.027</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.923</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.108</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.127</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.027\u001b[0m  \u001b[1;36m2.923\u001b[0m \u001b[1;36m-2.108\u001b[0m  \u001b[1;36m0.127\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.round(beta_T, 3).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.156</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.013</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.012</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.094</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.156\u001b[0m  \u001b[1;36m0.013\u001b[0m \u001b[1;36m-0.012\u001b[0m \u001b[1;36m-0.094\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.round(blackbox_model.coef_, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-bbcb9c1d1090>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accuracy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9910714285714286</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accuracy: \u001b[1;36m0.9910714285714286\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Precision: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9873506676036542</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Precision: \u001b[1;36m0.9873506676036542\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9950424929178471</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall: \u001b[1;36m0.9950424929178471\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">F1: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9911816578483246</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "F1: \u001b[1;36m0.9911816578483246\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_T = (prediction_function(test_X, beta_T) > 0.5).astype(int)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(test_y_tweet, y_pred_T)}\")\n",
    "print(f\"Precision: {precision_score(test_y_tweet, y_pred_T)}\")\n",
    "print(f\"Recall: {recall_score(test_y_tweet, y_pred_T)}\")\n",
    "print(f\"F1: {f1_score(test_y_tweet, y_pred_T)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with the black-box classifier\n",
    "\n",
    "The trained logistic regression model acheives a similar performance to the black-box classifier, with an accuracy of 0.99. The coefficients for the two models are:\n",
    "\n",
    "- Black-box classifier: $[[ 1.156, 0.013, -0.012, -0.094]]$\n",
    "- Manual logistic regression: $[[ 0.027,  2.923, -2.108,  0.127]]$\n",
    "\n",
    "Here, although the coefficients are different in magnitude, the performance of the two models is similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
